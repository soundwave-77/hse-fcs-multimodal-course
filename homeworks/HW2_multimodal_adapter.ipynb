{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "BoFhT78mpSGZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ДЗ2. Мультимодальный адаптер к Qwen3-0.6B"
      ],
      "metadata": {
        "id": "EzWpMk0BTfno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Описание задания**\n",
        "\n",
        "В этом задании вы подключите внешнюю модальность (аудио или изображение) к языковой модели Qwen3-0.6B через небольшой обучаемый адаптер. Веса Qwen и выбранного предобученного энкодера мы замораживаем, обучается только адаптер.\n",
        "\n",
        "Выберите один из треков:\n",
        "\n",
        "- Трек A (аудио → текст)\n",
        "\n",
        "Описываем аудио (AudioCaps) с помощью Qwen. Энкодер: `HuBERT` / `wav2vec2` / `Whisper Encoder`.\n",
        "\n",
        "- Трек B (изображение → текст)\n",
        "\n",
        "Описываем изображения (image captioning датасет) с помощью Qwen. Энкодер: любая vision-модель.\n",
        "\n",
        "**Задачи:**\n",
        "\n",
        "1.   Заморозить параметры `QWEN` и предобученного энкодера (аудио или vision).\n",
        "\n",
        "2.   Создать и обучить адаптер, который сжимает временную / пространственную размерность признаков и проецирует их в скрытое пространство `Qwen`.\n",
        "\n",
        "3.   Подготовить данные для обучения и валидации модели.\n",
        "\n",
        "4.   Реализовать и сравнить несколько стратегий пулинга в адаптере (например, сжатие временной размерности для аудио или пространственной - для изображений).\n",
        "\n",
        "5.   Использовать `BERTScore` для оценки качества модели на отложенном датасете.\n",
        "\n",
        "**Основные этапы задания**\n",
        "\n",
        "*   Подготовка данных: для \"аудио → текст\" загрузите датасет AudioCaps, если \"изображение → текст\", то загрузите датасет для image captioning (например, `Flickr8k`), обработайте данные и создайте DataLoader для батчевого обучения. Рекомендуется выполнить полную предобработку данных (прекомпьют), чтобы сократить время обработки на этапе обучения.\n",
        "*   Реализация адаптера: создайте класс, который сжимает последовательность векторов.\n",
        "*   Интеграция с `QWEN`: реализуйте обработку входов и передачу через `QWEN`.\n",
        "*   Обучение модели: настройте процесс обучения с использованием `Cross Entropy Loss` и teacher forcing.\n",
        "*   Оценка: вычислите BERTScore между сгенерированными и реальными текстами на валидационном датасете.\n",
        "\n",
        "> **Внимание!** Последующие ячейки с условиями оформлены (название классов и переменных, инструкциии и комментарии) в ключе работы по треку А (аудио → текст). Если вы выбираете работать с vision задачей, можете ориентироваться содержательно на представленные кодовые сниппеты, но видоизменять их под свою задачу."
      ],
      "metadata": {
        "id": "SOtDtoFXTmY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сеттинг"
      ],
      "metadata": {
        "id": "8ek97gBqjS9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jENUF19MYaAU",
        "outputId": "4b5d0c04-5ba2-4d30-bcd0-b892c8dfce0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.1+cu121)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets torchaudio\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JbyKDHKueNBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMNWATzFYh67",
        "outputId": "cc5b8b5d-c26f-430d-9784-ab41c92ccf61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка данных**"
      ],
      "metadata": {
        "id": "qJ8xbIDFkYy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если вы выбираете трек с аудио, используйте датасет `AudioCaps`."
      ],
      "metadata": {
        "id": "iMH2kpEDbCmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# (╯°□°）╯︵ ┻━┻"
      ],
      "metadata": {
        "id": "H-IwBsjZ8oBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1. Создание и обучение AudioConvAdapter (3 балла)\n",
        "\n",
        "1. Загрузите модель **Qwen3-0.6B** и заморозьте её параметры.  \n",
        "2. Загрузите предобученный аудио-энкодер (параметры также должны быть заморожены). Вы можете выбрать одну из следующих моделей: **HuBERT**, **wav2vec2**, или **Whisper Encoder**. Учтите особенности выбранной модели:  \n",
        "   - Например, `wav2vec2-large-960h-lv60-self` использует `flash_attention`, которая работает только с GPU архитектуры Ampere и с типом данных float16. Это может вызвать сложности, если используется другое оборудование.  \n",
        "3. Реализуйте класс `AudioConvAdapter`, который уменьшает размерность последовательности аудио по времени и переводит её в текстовое пространство модели QWEN."
      ],
      "metadata": {
        "id": "-YGacOadeVci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_model_name = \"Qwen/Qwen3-0.6B\"\n",
        "qwen_tokenizer =  # your code here\n"
      ],
      "metadata": {
        "id": "4tYDMIDwYkAz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_model_name = # your code here\n"
      ],
      "metadata": {
        "id": "4hEbt8GXFhaP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: имплементируйте конструктор и метод forward. Добавьте необходимые аргументы в конструктор\n",
        "\n",
        "class AudioConvAdapter(nn.Module):\n",
        "    \"\"\"\n",
        "    Пример максимально упрощённого адаптера из 4 блоков:\n",
        "    relu(Conv1D(in, in)) -> Linear(in, hid) -> relu(Conv1D(hid, hid)) -> Linear(hid, qwen_in)\n",
        "    где in - размер аудио вектора\n",
        "    hid - скрытое состояние адаптера (hid > in)\n",
        "    qwen_in - размерность хиддена qwen\n",
        "      - первый Conv1D уменьшает число временных шагов (stride/pooling),\n",
        "      - затем Linear преобразует hidden_dim,\n",
        "      - потом снова Conv1D (доп. pooling),\n",
        "      - потом Linear подгоняет к нужной размерности.\n",
        "    Можно без линейных слоев увеличивать размерность, но получится больше параметров\n",
        "    Можете реализивать любую свою архитектуру.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        pass #TODO: your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dtype != torch.float32:\n",
        "            x = x.to(torch.float32)\n",
        "\n",
        "        #TODO: your code here\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ckqo6GCxcmfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_hidden_size = qwen_model.config.hidden_size\n",
        "\n",
        "print(qwen_hidden_size)\n",
        "print(audio_encoder.config.hidden_size)"
      ],
      "metadata": {
        "id": "gSzOFB4Wbtao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_hidden_dim =   # внутренняя размерность adapter’а\n",
        "audio_adapter = # your code here\n",
        "\n",
        "print(\"Trainable parameters in adapter:\", sum(p.numel() for p in audio_adapter.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "PoXrO9YFcs2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2. Подготовка данных (2 балла)"
      ],
      "metadata": {
        "id": "BoFhT78mpSGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_tsv_file(tsv_in, tsv_out):\n",
        "    with open(tsv_in, \"r\", encoding=\"utf-8\") as fin, open(tsv_out, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for line in fin:\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) > 4:\n",
        "                fixed_line = \"\\t\".join(parts[:3]) + \"\\t\" + \" \".join(parts[3:]).strip()\n",
        "                fout.write(fixed_line + \"\\n\")\n",
        "            else:\n",
        "                fout.write(line)\n",
        "\n",
        "fix_tsv_file(\"/content/audiocaps/audiocaps/audiocaps_train.tsv\",\n",
        "             \"/content/audiocaps/audiocaps/audiocaps_train_fixed.tsv\")"
      ],
      "metadata": {
        "id": "XGP_y2mFBxxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioCapsDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tsv_path: str,\n",
        "        root_dir: str,\n",
        "        max_audio_length: int = 16000 * 10,  # 10 секунд при 16kHz\n",
        "        target_sample_rate: int = 16000,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        tsv_path: путь к .tsv (train/val), содержащему столбцы: 'audio' и 'text'.\n",
        "        root_dir: корневая папка, содержащая файлы.\n",
        "        max_audio_length: ограничение по длине в сэмплах (обрезаем длинные аудио).\n",
        "        target_sample_rate: ожидаемая частота дискретизации.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "        self.root_dir = root_dir\n",
        "        self.max_audio_length = max_audio_length\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Реализуйте чтение обучаеющего примера.\n",
        "        # Обратите внимание, что частота дискретизации (Sample Rate) должна соотвествовать частоте, на которой обучался аудио энкодер.\n",
        "        # Для ускорения обучения можно заранее отресемплить и векторизовать аудио, чтобы не тратить компьюь при обучении.\n",
        "        # Для дебага наоборот, проще на лету, чтобы не ждать долгую стадию препроцессинга.\n",
        "        # В этой стадии можно ограничить длительность аудио, например, 10 сек\n",
        "        # return waveform, sr, caption\n",
        "        # return vectorized_audio, caption\n",
        "\n",
        "#реализуйте один из вариантов collate_fn для пайплайна с процессингом оффлайн или на лету\n",
        "\n",
        "# def collate_fn(batch, audio_processor: Wav2Vec2Processor):\n",
        "#     \"\"\"\n",
        "#     batch: список из N элементов [(waveform_i, sr_i, caption_i), ...].\n",
        "#     Делает единый вызов audio_processor(..., padding=\"longest\") для всего батча,\n",
        "#     возвращает (audio_inputs, captions).\n",
        "#     \"\"\"\n",
        "\n",
        "#     # audio_inputs[\"input_values\"] => форма [B, T_max]\n",
        "#     # audio_inputs[\"attention_mask\"] => форма [B, T_max]\n",
        "\n",
        "#     return audio_inputs, captions\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     \"\"\"\n",
        "#     batch: список из N элементов [(waveform_i, sr_i, caption_i), ...].\n",
        "#     Делает единый вызов audio_processor(..., padding=\"longest\") для всего батча,\n",
        "#     возвращает (audio_inputs, captions).\n",
        "#     \"\"\"\n",
        "\n",
        "#     # audio_inputs[\"input_values\"] => форма [B, T_max]\n",
        "#     # audio_inputs[\"attention_mask\"] => форма [B, T_max]\n",
        "\n",
        "#     return audio_inputs, captions\n",
        "\n",
        "train_tsv = \"/content/audiocaps/audiocaps/audiocaps_train_fixed.tsv\"\n",
        "val_tsv   = \"/content/audiocaps/audiocaps/audiocaps_val_new.tsv\"\n",
        "\n",
        "root_dir = \"/content/audiocaps\""
      ],
      "metadata": {
        "id": "rnlU_CsvWwV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AudioCapsDataset(train_tsv, root_dir)\n",
        "val_dataset   = AudioCapsDataset(val_tsv, root_dir)\n",
        "# инициализируйте Data loader'ы\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=# your collate\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=# your collate\n",
        ")"
      ],
      "metadata": {
        "id": "dhVAzKNExaze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка\n",
        "\n",
        "for batch in train_loader:\n",
        "    audio_inputs, captions = batch\n",
        "    print(\"input_values.shape =\", audio_inputs[\"input_values\"].shape)  # [B, T_max]\n",
        "    print(\"attention_mask.shape =\", audio_inputs[\"attention_mask\"].shape)  # [B, T_max]\n",
        "    print(\"captions =\", captions)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdT63ULkxXIA",
        "outputId": "a0e46cfb-b204-4a6a-f5eb-f5c13ef0de52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_values.shape = torch.Size([4, 160000])\n",
            "attention_mask.shape = torch.Size([4, 160000])\n",
            "captions = ['A man speaking with distant murmuring and clanking', 'Several cat meows', 'A person talking and dribbling a basketball', 'A very aggressive sounding dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3. Трейн QwenAudioDescription (3 балла)\n"
      ],
      "metadata": {
        "id": "7IJPPyqspYeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите класс `QwenAudioDescriptionTrainer`, который будет включать в себя:\n",
        "   - Обучение адаптера (`train_one_epoch`).\n",
        "   - Валидацию (`validate`).\n",
        "   - Генерацию описания аудио (`generate`).\n",
        "2. Реализуйте процесс обучения, который объединяет аудио-эмбеддинги и текстовые токены, а затем передаёт их в QWEN для предсказания текстов.\n",
        "3. Используйте Cross Entropy Loss для оптимизации аудио-адаптера. Остальные параметры модели остаются замороженными.\n"
      ],
      "metadata": {
        "id": "qJfYjomHc-wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создайте ID для специального токена [AUDIO]\n",
        "audio_token_id = qwen_tokenizer(\"[AUDIO]\", add_special_tokens=False)[\"input_ids\"][0]\n",
        "special_token_id = audio_token_id\n",
        "\n",
        "class QwenAudioDescriptionTrainer:\n",
        "    def __init__(self, qwen_model, qwen_tokenizer, audio_encoder, audio_adapter, lr=1e-4):\n",
        "        self.qwen_model = qwen_model\n",
        "        self.qwen_tokenizer = qwen_tokenizer\n",
        "        self.audio_encoder = audio_encoder\n",
        "        self.audio_adapter = audio_adapter\n",
        "\n",
        "        # Создайте оптимизатор Adam (только для audio_adapter)\n",
        "        # self.optimizer = ...\n",
        "\n",
        "    def train_one_epoch(self, train_loader):\n",
        "        self.audio_adapter.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            audio_inputs, texts = batch  # (audio_inputs, список строк)\n",
        "\n",
        "            # Подготовьте input_values и attn_mask - маску для аттеншена (только аудио, тк текст предсказываем)\n",
        "\n",
        "            # *Этот шаг выполняется, если аудио векторы не вычислялись при формировании батча.\n",
        "            # *Прогните через audio_encoder (заморожен) без градиентов\n",
        "            with torch.no_grad():\n",
        "                # *audio_hidden_states = ...\n",
        "                pass\n",
        "\n",
        "            # Пропустите скрытые состояния через аудио-адаптер\n",
        "            # Токенизируйте текстовые данные и примите QWEN эмбеддер\n",
        "            # Соберите all_embeddings для модели cat(audio, text)\n",
        "            # Таргетом являются лейблы текстовых токенов (описаний аудио)\n",
        "            # Пропустите данные через модель QWEN и вычислите лосс\n",
        "            # Выполните шаг оптимизации\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        self.audio_adapter.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "                audio_inputs, texts = batch\n",
        "\n",
        "                # Аналогично train, но без backward\n",
        "                # your code here\n",
        "\n",
        "                pass\n",
        "\n",
        "        return val_loss / len(val_loader)\n",
        "\n",
        "    def generate(self, audio_inputs):\n",
        "        \"\"\"\n",
        "        Генерация описания для одного аудио.\n",
        "        \"\"\"\n",
        "        self.audio_adapter.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # 1) input_values + attn_mask\n",
        "            # 2) audio_encoder -> audio_adapter\n",
        "            # 3) Склейте audio_embeds + небольшой pseudo_input_ids\n",
        "            # 4) Сгенерируйте текст\n",
        "            # generated_ids = ...\n",
        "            # generated_text = ...\n",
        "\n",
        "        return generated_text\n"
      ],
      "metadata": {
        "id": "Vq4Kj9DNo4Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = QwenAudioDescriptionTrainer(...)\n",
        "\n",
        "# your code here\n",
        "# ┌(ಠ_ಠ)┘\n"
      ],
      "metadata": {
        "id": "Al6T5xqfWjA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Убедиться, что:\n",
        ">\n",
        "> При обучении лосс падает.\n",
        ">\n",
        "> При валидации всё аналогично, только без backward.\n",
        ">\n",
        ">  При генерации появляется текст (возможно, не самый качественный - это зависит от данных и количества эпох)."
      ],
      "metadata": {
        "id": "Cabil9ZVpDiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4. Валидация (2 балла)"
      ],
      "metadata": {
        "id": "lZfOOuuephD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Используйте валидационный набор данных, чтобы проверить, насколько хорошо модель генерирует текстовые описания для аудио/картинок.\n",
        "\n",
        "2. Реализуйте процесс генерации текстов для всех аудио/картинок из валидационного набора.\n",
        "\n",
        "3. Используйте метрику **BERTScore** для оценки качества сгенерированных описаний.\n",
        "\n",
        "4. Отобразите примеры сгенерированных текстов и сравните их с истинными описаниями (references)."
      ],
      "metadata": {
        "id": "GdPZsMIvr6-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "\n",
        "# your code here (＠_＠)"
      ],
      "metadata": {
        "id": "rGwRYSEmM8np"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}